{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV-Tutorials 3- Image Processing Techniques\n",
    "\n",
    "The value of pixels will be the features for a Deep learning model. So it is important to give the model only the important features. So we need to remove noise, transform the image, smooth the image, etc to get a better result.\n",
    "OpenCV provides below techniques to process the pixels of the image:\n",
    "\n",
    "__Contents:__\n",
    "<ol>\n",
    "    <li>Thresholding</li>\n",
    "    <li>Color Filters</li>\n",
    "    <li>Smoothing-Denoising transformations(To remove noise)</li>\n",
    "    <li>Blurring (To remove noise)</li>\n",
    "</ol>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/dogs/bookpage.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    cv2.startWindowThread()\n",
    "    cv2.namedWindow(\"preview\")\n",
    "    cv2.imshow('image',img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1\n",
    "## 1.Thresholding:\n",
    "\n",
    "<br>There are 2 types of thresholding:\n",
    "<br>1.Global Thresholding\n",
    "<br>2.Adaptive Thresholding\n",
    "<br>\n",
    "\n",
    "1.__Global Thresholding__:\n",
    "We select a value for threshold. If the pixel value is greater than some threshold, we change the pixel value to something else\n",
    "\n",
    "2.__Adaptive Thresholding__:\n",
    "The algorithm automatically selects the threshold as the image could need different thresholds at different places\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Thresholding:\n",
    "__cv2.threshold(image,threshold_value,upgrade_value,type_of_threshold)__ returns the image_array with values thresholded\n",
    "\n",
    "<br>types of threshold:\n",
    "<br>cv2.THRESH_BINARY\n",
    "<br>cv2.THRESH_BINARY_INV\n",
    "<br>cv2.THRESH_TRUNC\n",
    "<br>cv2.THRESH_TOZERO\n",
    "<br>cv2.THRESH_TOZERO_INV\n",
    "<br>\n",
    "We can add cv.THRESH_OTSU to these thresholds to select threshold automatically\n",
    "\n",
    "for detailed explanation, please visit https://docs.opencv.org/3.4.3/d7/d4d/tutorial_py_thresholding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/sample/noise.jpg',0)\n",
    "show_img(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_,new_image1) = cv2.threshold(image,12,255,cv2.THRESH_BINARY)\n",
    "show_img(new_image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_,new_image1) = cv2.threshold(image,12,255,cv2.THRESH_BINARY_INV)\n",
    "show_img(new_image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_,new_image1) = cv2.threshold(image,12,255,cv2.THRESH_TRUNC)\n",
    "show_img(new_image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_,new_image1) = cv2.threshold(image,12,255,cv2.THRESH_TOZERO)\n",
    "show_img(new_image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_,new_image1) = cv2.threshold(image,12,255,cv2.THRESH_TOZERO_INV)\n",
    "show_img(new_image1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets add cv2.THRESH_OTSU to threshold_type to select threshold automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold_type in [cv2.THRESH_BINARY,cv2.THRESH_BINARY_INV,cv2.THRESH_TRUNC,cv2.THRESH_TOZERO,cv2.THRESH_TOZERO_INV]:\n",
    "    show_img(cv2.threshold(image,12,255,threshold_type+cv2.THRESH_OTSU)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Thresholding:\n",
    "\n",
    "__adaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C)__ \n",
    "\n",
    "adaptiveMethods:\n",
    "<ol>\n",
    "    <li>cv2.ADAPTIVE_THRESH_MEAN_C</li>\n",
    "    <li>cv2.ADAPTIVE_THRESH_GAUSSIAN_C</li>\n",
    "</ol>\n",
    "\n",
    "thresholdTypes can be:\n",
    "<ol>\n",
    "    <li>cv2.THRESH_BINARY</li>\n",
    "    <li>cv2.THRESH_BINARY_INV</li>\n",
    "</ol>\n",
    "\n",
    "For details, please visit https://docs.opencv.org/3.4.3/d7/d4d/tutorial_py_thresholding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11,2)\n",
    "show_img(new_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets run all the possible combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adaptive_method in [cv2.ADAPTIVE_THRESH_MEAN_C,cv2.ADAPTIVE_THRESH_GAUSSIAN_C]:\n",
    "    for threshold_type in [cv2.THRESH_BINARY,cv2.THRESH_BINARY_INV]:\n",
    "        show_img(cv2.adaptiveThreshold(image,255,adaptive_method,threshold_type,11,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2\n",
    "## Color Filters:\n",
    "\n",
    "__cv2.inRange(hsv,lower_range_hsv,upper_range_hsv)__ retruns only pixels which are in range from lower range hsv value to upper range hsv value\n",
    "<br>\n",
    "\n",
    "\n",
    "to convert bgr to hsv values i.e.to calculate lower and upper bound hsv values use external website like\n",
    "https://www.rapidtables.com/convert/color/rgb-to-hsv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_filter(img,lower_range_hsv,upper_range_hsv):\n",
    "    hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "    return(cv2.inRange(hsv,lower_range_hsv,upper_range_hsv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_filter_fill(img,lower_range_hsv,upper_range_hsv):\n",
    "    hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "    mask = cv2.inRange(hsv,lower_range_hsv,upper_range_hsv)\n",
    "    return(cv2.bitwise_and(img,img,mask=mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0   0 255]]]\n"
     ]
    }
   ],
   "source": [
    "def hsv2bgr(hsv):\n",
    "    return(cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR))\n",
    "print(hsv2bgr(np.uint8([[[180,255,255]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(1)\n",
    "while True:\n",
    "    _,frame = capture.read()\n",
    "    cv2.imshow('original',frame)\n",
    "    cv2.imshow('filtered_mask',color_filter(frame,np.array([150,150,50]),np.array([180,255,180])))#filter red\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "capture.release()\n",
    "cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets fill the white area i.e. red in original image, white in masked image with original pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(1)\n",
    "while True:\n",
    "    _,frame = capture.read()\n",
    "    cv2.imshow('original',frame)\n",
    "    cv2.imshow('filtered_mask',color_filter(frame,np.array([150,150,50]),np.array([180,255,180])))#filter red\n",
    "    cv2.imshow('filtered_mask',color_filter_fill(frame,np.array([150,150,50]),np.array([180,255,180])))#filter red\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "capture.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/color_filter_red.png\" style=\"width:100px;height:100;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-3\n",
    "## Denoising(smoothing) the image\n",
    "\n",
    "<ol>\n",
    "    <li>Erosion</li>\n",
    "    <li>Dilation</li>\n",
    "    <li>combinations of erosion,dilation and original image as Morphological transformations</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/sample/noise.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erosion:\n",
    "It erodes away the boundaries of foreground object (Always try to keep foreground in white). So what it does? The kernel slides through the image (as in 2D convolution). A pixel in the original image (either 1 or 0) will be considered 1 only if all the pixels under the kernel is 1, otherwise it is eroded (made to zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((5,5),np.uint8)\n",
    "eroded_image = cv2.erode(image,kernel,iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(image)\n",
    "show_img(eroded_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation:\n",
    "It is just opposite of erosion. Here, a pixel element is '1' if atleast one pixel under the kernel is '1'. So it increases the white region in the image or size of foreground object increases. Normally, in cases like noise removal, erosion is followed by dilation. Because, erosion removes white noises, but it also shrinks our object. So we dilate it. Since noise is gone, they won't come back, but our object area increases. It is also useful in joining broken parts of an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilated_image = cv2.dilate(image,kernel,iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(image)\n",
    "show_img(dilated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphologic Transformations\n",
    "\n",
    "__cv2.morphologyEx(image, morphology_type , kernel)__ returns array of transformed image\n",
    "\n",
    "morphology_type can be :\n",
    "<ol>\n",
    "    <li>cv2.MORPH_OPEN</li>\n",
    "    <li>cv2.MORPH_CLOSE</li>\n",
    "    <li>cv2.MORPH_GRADIENT</li>\n",
    "    <li>cv2.MORPH_TOPHAT</li>\n",
    "    <li>cv2.MORPH_BLACKHAT</li>\n",
    "</ol>\n",
    "<br>\n",
    "kernel can be :\n",
    "<ol>\n",
    "    <li>__Rectangular kernel__: cv2.getStructuringElement(cv2.MORPH_RECT,(rows,colums))</li>\n",
    "    <li>__Elliptical Kernel__: cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(rows,columns))</li>\n",
    "    <li>__Cross-shaped Kernel__: cv2.getStructuringElement(cv2.MORPH_CROSS,(rows,columns))</li>\n",
    "    <li>__random matrix shape less than image__: np.ones((5,5),np.uint8)/225\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "When to apply morphology_type: \n",
    "<br>__cv2.MORPH_OPEN__ : Opening is just another name of erosion followed by dilation. It is useful in removing noise\n",
    "<br>__cv2.MORPH_CLOSE__ : Closing is reverse of Opening, Dilation followed by Erosion. It is useful in closing small holes inside the foreground objects, or small black points on the object.\n",
    "<br>__cv2.MORPH_GRADIENT__: It is the difference between dilation and erosion of an image.\n",
    "<br>__cv2.MORPH_TOPHAT__ : It is the difference between input image and Opening of the image\n",
    "<br>__cv2.MORPH_BLACKHAT__ : It is the difference between the closing of the input image and input image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphologic_image = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(image)\n",
    "show_img(morphologic_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for morphological_type in [cv2.MORPH_OPEN,cv2.MORPH_CLOSE,cv2.MORPH_GRADIENT,cv2.MORPH_TOPHAT,cv2.MORPH_BLACKHAT]:\n",
    "    for kernel in [cv2.getStructuringElement(cv2.MORPH_RECT,(5,5)),cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5)),cv2.getStructuringElement(cv2.MORPH_CROSS,(5,5)),np.ones((5,5),np.uint8)/225]:\n",
    "        show_img(image)\n",
    "        show_img(cv2.morphologyEx(image, morphological_type, kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4\n",
    "## Blurring\n",
    "\n",
    "<ol>\n",
    "    <li>filter2d</li>\n",
    "    <li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/sample/noise.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((5,5),np.uint8)/255\n",
    "filtered_image = cv2.filter2D(image,-1,kernel)\n",
    "show_img(filtered_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(image)\n",
    "show_img(cv2.blur(image,(5,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(image)\n",
    "show_img(cv2.GaussianBlur(image,(5,5),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
